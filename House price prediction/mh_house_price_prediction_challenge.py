# -*- coding: utf-8 -*-
"""mh House Price Prediction Challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/157S9g1g3Czojpnn37amZ55dLrv39vWLc
"""

!wget https://machinehack-be.s3.amazonaws.com/house_price_prediction_beat_the_benchmark/Participants_Data_HPP.zip

!unzip Participants_Data_HPP.zip

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

train=pd.read_csv('Participants_Data_HPP/Train.csv')
test=pd.read_csv('Participants_Data_HPP/Test.csv')

train.head()

train.isnull().sum(),test.isnull().sum(),train.shape,test.shape,train.dtypes

df=train.append(test,ignore_index=True)

df.nunique()

#df = pd.get_dummies(df, columns=['POSTED_BY','BHK_OR_RK'])

df['ad_1'] = df['ADDRESS'].str.split(',').str[0]
df['ad_2'] = df['ADDRESS'].str.split(',').str[1]
df['ad_2'] = df['ad_2'].fillna('missingADDRESS')

# back
x=[]
m=list(df['ad_2'])
for i in df['ad_2']:
  k=m.count(i)
  if k<3:
    x.append(i)
  else:
    pass

df['ad_2'].shape

i=list(range(0, 98171))
df['title']=i

import re
go_tags=df.copy()
temp=go_tags['ad_2'].str.split(',')

go_tags=go_tags.reindex(go_tags.index.repeat(temp.apply(len)))
go_tags['ad_2_new'] = np.hstack(temp)
go_tags=go_tags[['title','ad_2_new']]
go_tags=pd.crosstab(go_tags.title,go_tags.ad_2_new)
go_tags.head()

df=df.merge(go_tags, on='title', how='left')

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['ad_1'] = le.fit_transform(df['ad_1'])
df['ad_1']=df['ad_1']+1
le = LabelEncoder()
df['ad_2'] = le.fit_transform(df['ad_2'])
df['ad_2']=df['ad_2']+1
del df['ADDRESS']

#back
for i in df.columns:
  if i in x:
    del df[i]
  else:
    pass

df['SQUARE_FT_r']=df['SQUARE_FT']/df['BHK_NO.']
df['SQUARE_FT_rrb']=df['SQUARE_FT']/(df['READY_TO_MOVE']+df['RESALE'])

df['POSTED_BY'] = le.fit_transform(df['POSTED_BY'])
df['BHK_OR_RK'] = le.fit_transform(df['BHK_OR_RK'])

train = df[df['TARGET(PRICE_IN_LACS)'].isnull()==False]
test = df[df['TARGET(PRICE_IN_LACS)'].isnull()==True]
del test['TARGET(PRICE_IN_LACS)']

train_df=train.copy()
test_df=test.copy()

train_df['TARGET(PRICE_IN_LACS)'] = np.log1p(train_df['TARGET(PRICE_IN_LACS)'])

X = train_df.drop(labels=['TARGET(PRICE_IN_LACS)'], axis=1)
y = train_df['TARGET(PRICE_IN_LACS)'].values

from sklearn.model_selection import train_test_split
X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.25, random_state=42)

X_train.shape, y_train.shape, X_cv.shape, y_cv.shape

from math import sqrt 
from sklearn.metrics import mean_squared_log_error

"""# lgb"""

import lightgbm as lgb
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_cv, label=y_cv)

param = {'objective': 'regression',
         'boosting': 'gbdt',  
         'metric': 'l2_root',
         'learning_rate': 0.1, 
         'num_iterations': 1000,
         'num_leaves': 69,
         'max_depth': -1,
         'min_data_in_leaf': 12,
         'bagging_fraction': 0.80,
         'bagging_freq': 1,
         'bagging_seed': 3,
         'feature_fraction': 0.90,
         'feature_fraction_seed': 2,
         'early_stopping_round': 200,
         'max_bin': 250
         }

lgbm = lgb.train(params=param, verbose_eval=100, train_set=train_data, valid_sets=[test_data])

y_pred_lgbm = lgbm.predict(X_cv)
print('RMSLE:', sqrt(mean_squared_log_error(np.expm1(y_cv), np.expm1(y_pred_lgbm))))

import seaborn as sns
feature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importance(), X.columns), reverse=True)[:50], 
                           columns=['Value','Feature'])
plt.figure(figsize=(12, 10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False))
plt.title('LightGBM Features')
plt.tight_layout()
plt.show()

Xtest = test_df

from sklearn.model_selection import KFold
from lightgbm import LGBMRegressor

errlgb = []
y_pred_totlgb = []

fold = KFold(n_splits=4, shuffle=True, random_state=101)

for train_index, test_index in fold.split(X):
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = y[train_index], y[test_index]

    lgbm = LGBMRegressor(**param)
    lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=200)

    y_pred_lgbm = lgbm.predict(X_test)
    print("RMSLE LGBM: ", sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_lgbm))))

    errlgb.append(sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_lgbm))))
    p = lgbm.predict(Xtest)
    y_pred_totlgb.append(p)

np.mean(errlgb,0)

lgbm_final = np.expm1(np.mean(y_pred_totlgb,0))
lgbm_final

submission = pd.DataFrame({
        "TARGET(PRICE_IN_LACS)":lgbm_final
    })
submission.to_csv('./mhpno.csv', index=False)
print(submission)



"""# gb"""

from sklearn.ensemble import GradientBoostingRegressor
gb = GradientBoostingRegressor(verbose=1, learning_rate=0.1, n_estimators=1000, random_state=42, subsample=0.8)
gb.fit(X_train, y_train)
y_pred = gb.predict(X_cv)
print('RMSLE:', sqrt(mean_squared_log_error(np.expm1(y_cv), np.expm1(y_pred))))

feature_imp = pd.DataFrame(sorted(zip(gb.feature_importances_, X.columns), reverse=True)[:60], columns=['Value','Feature'])
plt.figure(figsize=(12,10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False))
plt.title('Gradient Boosting Features')
plt.tight_layout()
plt.show()

Xtest = test_df

from sklearn.model_selection import KFold

errgb = []
y_pred_totgb = []

fold = KFold(n_splits=4, shuffle=True, random_state=42)

for train_index, test_index in fold.split(X):
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = y[train_index], y[test_index]
             
    gb = GradientBoostingRegressor(learning_rate=0.10, n_estimators=1000, random_state=42, subsample=0.8)
    #gb = GradientBoostingRegressor(learning_rate=0.1, n_estimators=650, random_state=42, subsample=0.8)
    gb.fit(X_train, y_train)
    y_pred = gb.predict(X_test)

    print('MSE', sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred))))

    errgb.append(sqrt(mean_squared_log_error(np.exp(y_test),np.exp(y_pred))))
    p = gb.predict(Xtest)
    y_pred_totgb.append(p)

np.mean(errgb,0)

gb_fib = np.expm1(np.mean(y_pred_totgb,0))
gb_fib



"""# cat"""

!pip install catboost

from catboost import CatBoostRegressor
cat = CatBoostRegressor(verbose=100, #learning_rate=0.4, 
                        n_estimators=10000, random_state=42, subsample=0.9, task_type='GPU')
cat.fit(X_train, y_train)
y_pred = cat.predict(X_cv)
print('RMSLE', sqrt(mean_squared_log_error(np.expm1(y_cv), np.expm1(y_pred))))

import seaborn as sns
feature_imp = pd.DataFrame(sorted(zip(cat.feature_importances_, X.columns), reverse=True)[:60], columns=['Value','Feature'])
plt.figure(figsize=(12,10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False))
plt.title('Gradient Boosting Features')
plt.tight_layout()
plt.show()

Xtest=test_df

from xgboost import XGBRegressor
from sklearn.model_selection import KFold

errxgb = []
y_pred_totxgb = []

fold = KFold(n_splits=10, shuffle=True, random_state=42)

for train_index, test_index in fold.split(X):
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    #xgb = XGBRegressor(random_state=101)
    xgb=XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror', max_depth=9, n_estimators=1000,tree_method='gpu_hist')# )
    xgb.fit(X_train, y_train)

    y_pred_xgb = xgb.predict(X_test)
    print("RMSLE: ", sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_xgb))))

    errxgb.append(sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_xgb))))
    p = xgb.predict(Xtest)

    y_pred_totxgb.append(p)

np.mean(errxgb,0)

final1 = np.expm1(np.mean(y_pred_totxgb,0))
final1

submission = pd.DataFrame({
        "TARGET(PRICE_IN_LACS)":final1
    })
submission.to_csv('./styl0e.csv', index=False)
print(submission)

